{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOMO - Faster Objects, More Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import Resize\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "from torchmetrics import MetricCollection, MeanAbsoluteError, MeanSquaredError\n",
    "from torchmetrics.classification import BinaryF1Score\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import MLFlowLogger\n",
    "\n",
    "from monai.losses.dice import DiceLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = Path(\"../vehicle_dataset_split\")\n",
    "\n",
    "IMAGE_RESOLUTION = 448  # model input image resolution (assume input is squared shape)\n",
    "GRID_SCALE = 8          # number of pixels per cell on prediction grid (vertically and horizontally)\n",
    "NUMBER_OF_CLASSES = 1\n",
    "TEST_DATASET = False\n",
    "\n",
    "NUMBER_OF_EPOCHS = 1\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "DETECTION_THRESHOLD = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BoundingBoxToMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundingBoxToMask:\n",
    "    \"\"\"\n",
    "    Convert YOLO bounding boxes to FOMO centroids. Used in label processing.\n",
    "    Return mask of centroids and number of objects in mask.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_resolution: int, grid_scale: int, number_of_classes: int):\n",
    "        self._image_resolution = image_resolution\n",
    "        self._grid_scale = grid_scale\n",
    "        self._number_of_classes = number_of_classes\n",
    "\n",
    "    def __call__(self, label):\n",
    "        grid_resolution = int(self._image_resolution / self._grid_scale)\n",
    "\n",
    "        masks = []\n",
    "        count = []\n",
    "        for i in range(self._number_of_classes):\n",
    "            class_mask = torch.zeros((grid_resolution, grid_resolution), dtype=float)\n",
    "            bounding_boxes = label.readlines()\n",
    "            for bounding_box in bounding_boxes:\n",
    "                [c, x, y, _, _] = [\n",
    "                    float(coords) for coords in bounding_box.split()\n",
    "                ]  # get bounding box class, position and dimension\n",
    "                if c == i:\n",
    "                    x = int(x * grid_resolution)\n",
    "                    y = int(y * grid_resolution)\n",
    "                    class_mask[y, x] = 1.0\n",
    "            masks.append(class_mask)\n",
    "            count.append(len(bounding_boxes))\n",
    "\n",
    "        return torch.stack(masks), count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VehicleDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_dir: Path,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "    ):\n",
    "        self._images = sorted([\n",
    "            element for element in (dataset_dir / Path(\"images\")).iterdir()\n",
    "        ])\n",
    "        self._labels = sorted([\n",
    "            element for element in (dataset_dir / Path(\"bounding_boxes\")).iterdir()\n",
    "        ])\n",
    "        self._transform = transform\n",
    "        self._target_transform = target_transform\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        image_path = self._images[idx]\n",
    "        image = read_image(str(image_path))\n",
    "        image = self._transform(image.float())\n",
    "\n",
    "        label_path = self._labels[idx]\n",
    "        label_file = open(label_path)\n",
    "        centroid_mask, count = self._target_transform(label_file)\n",
    "\n",
    "        return image, centroid_mask, count\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VehicleDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_dir: Path,\n",
    "        input_resolution: int,\n",
    "        grid_scale: int,\n",
    "        batch_size: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._dataset_dir = dataset_dir\n",
    "        self._input_resolution = input_resolution  \n",
    "        self._grid_scale = grid_scale\n",
    "        self._batch_size = batch_size\n",
    " \n",
    "        self._transform = Resize((input_resolution, input_resolution), antialias=True)\n",
    "        self._target_transform = BoundingBoxToMask(input_resolution, grid_scale, NUMBER_OF_CLASSES)\n",
    "        self._train_dataset = None\n",
    "        self._val_dataset = None\n",
    "        self._test_dataset = None\n",
    "        self._test_dataset = None\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        self._train_dataset = VehicleDataset(\n",
    "            self._dataset_dir / Path(\"training\"),\n",
    "            self._transform,\n",
    "            self._target_transform,\n",
    "        )\n",
    "        self._val_dataset = VehicleDataset(\n",
    "            self._dataset_dir / Path(\"validation\"),\n",
    "            self._transform,\n",
    "            self._target_transform,\n",
    "        )\n",
    "        self._test_dataset = VehicleDataset(\n",
    "            self._dataset_dir / Path(\"test\" if TEST_DATASET else \"validation\"),\n",
    "            self._transform,\n",
    "            self._target_transform,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self._train_dataset, batch_size=self._batch_size, num_workers=7)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self._val_dataset, batch_size=self._batch_size, num_workers=7)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self._test_dataset, batch_size=self._batch_size, num_workers=7)\n",
    "\n",
    "vehicle_data_module = VehicleDataModule(\n",
    "    dataset_dir=DATASET_DIR,\n",
    "    input_resolution=IMAGE_RESOLUTION,\n",
    "    grid_scale=GRID_SCALE,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FOMO network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fomo_net(weights: None, number_of_classes: int) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Constructs a FOMO net model with a specified number of classes.\n",
    "    Input resolution is decreased by a factor of 8.\n",
    "    \"\"\"\n",
    "    model = mobilenet_v2(weights=weights)\n",
    "\n",
    "    # Remove layers after 1/8 size reduction\n",
    "    cut_inverted_residual = 7\n",
    "    features = [\n",
    "        nn.Identity() if i > cut_inverted_residual else model.features[i]\n",
    "        for i, _ in enumerate(model.features)\n",
    "    ]\n",
    "    features[cut_inverted_residual].conv[1] = nn.Identity()\n",
    "    features[cut_inverted_residual].conv[2] = nn.Identity()\n",
    "    features[cut_inverted_residual].conv[3] = nn.Identity()\n",
    "    model = nn.Sequential(*features)\n",
    "    \n",
    "    # Add classification layers\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Conv2d(\n",
    "            in_channels=32*6, \n",
    "            out_channels=32, \n",
    "            kernel_size=1, \n",
    "            stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(\n",
    "            in_channels=32,\n",
    "            out_channels=number_of_classes,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "        ),\n",
    "    )    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funtion to postprocess the mask generated by FOMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_prediction(\n",
    "    mask_pred: torch.Tensor, threshold: float, binary: bool=True, connected: bool=True\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Postprocesses FOMO mask prediction to squish output between [0:1] and remove multiple detections of the same object\n",
    "    \"\"\"\n",
    "    mask_pred = torch.sigmoid(mask_pred)\n",
    "    obj_cnt = []\n",
    "    for single_mask_pred in mask_pred.unbind(0):\n",
    "        for i, channel in enumerate(single_mask_pred.unbind(0)):\n",
    "            channel = channel.numpy()\n",
    "            if binary:\n",
    "                binary_channel = np.where(channel > threshold, 1.0, 0.0).astype(np.int8)\n",
    "            else:\n",
    "                binary_channel = channel\n",
    "            if connected:\n",
    "                number_of_blobs, blobs = cv2.connectedComponentsWithAlgorithm(\n",
    "                    binary_channel,\n",
    "                    connectivity=8,\n",
    "                    ltype=cv2.CV_32S,\n",
    "                    ccltype=cv2.CCL_WU,\n",
    "                )\n",
    "                binary_channel.fill(0)\n",
    "                for blob in range(1, number_of_blobs):\n",
    "                    blob_mask = (blobs == blob)\n",
    "                    max_val = np.max(channel[blob_mask])\n",
    "                    channel_max = np.where((channel == max_val), 1, 0)\n",
    "                    binary_channel += np.logical_and(channel_max, blob_mask)\n",
    "                obj_cnt.append(number_of_blobs)\n",
    "\n",
    "            single_mask_pred[i] = torch.from_numpy(binary_channel)\n",
    "    return mask_pred, torch.IntTensor(obj_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FomoModule(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        # Network architecture\n",
    "        self._network = fomo_net(\n",
    "            weights=MobileNet_V2_Weights.DEFAULT,\n",
    "            number_of_classes=NUMBER_OF_CLASSES,\n",
    "        )\n",
    "        # Loss function\n",
    "        self._loss_function = DiceLoss(sigmoid=True)\n",
    "        # Metrics\n",
    "        metrics = MetricCollection([\n",
    "            BinaryF1Score(),\n",
    "        ])\n",
    "        cnt_metrics = MetricCollection([\n",
    "            MeanAbsoluteError(),\n",
    "            MeanSquaredError()\n",
    "        ])\n",
    "        self._metrics = {\n",
    "            \"classification\":{\n",
    "                \"train\": metrics.clone(prefix='train_'),\n",
    "                \"valid\": metrics.clone(prefix='val_'),\n",
    "                \"test\": metrics.clone(prefix='test_')\n",
    "            },\n",
    "            \"counting\":{\n",
    "                \"train\": cnt_metrics.clone(prefix='train_'),\n",
    "                \"valid\": cnt_metrics.clone(prefix='val_'),\n",
    "                \"test\": cnt_metrics.clone(prefix='test_')\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        return self._network(inputs)\n",
    "\n",
    "    def step(self, batch, type: str):\n",
    "        image, mask, cnt = batch\n",
    "        mask_pred = self(image)\n",
    "        loss = self._loss_function(mask_pred, mask)\n",
    "        mask_pred, cnt_pred = postprocess_prediction(\n",
    "            mask_pred.detach().cpu(), threshold=DETECTION_THRESHOLD\n",
    "        )\n",
    "        metric = self._metrics[\"classification\"][type](mask_pred, mask.cpu())\n",
    "        cnt_metric = self._metrics[\"counting\"][type](cnt_pred, cnt[0].cpu())\n",
    "        return loss, metric, cnt_metric\n",
    "\n",
    "    def log_metrics(self, loss, metric, cnt_metric, type: str):\n",
    "        self.log(type + \"_loss\", loss, prog_bar=True)\n",
    "        self.log_dict(metric)\n",
    "        self.log_dict(cnt_metric)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, metric, cnt_metric = self.step(batch, \"train\")\n",
    "        self.log_metrics(loss, metric, cnt_metric, \"train\")\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, metric, cnt_metric = self.step(batch, \"valid\")\n",
    "        self.log_metrics(loss, metric, cnt_metric, \"valid\")\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, metric, cnt_metric = self.step(batch, \"test\")\n",
    "        self.log_metrics(loss, metric, cnt_metric, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=LEARNING_RATE)\n",
    "        scheduler = MultiStepLR(optimizer, milestones=[100, 125], gamma=0.1)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "fomo_module = FomoModule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_logger = MLFlowLogger(\n",
    "    experiment_name=\"FOMO_Crossroad\",\n",
    "    run_name= str(IMAGE_RESOLUTION) + '/' + str(GRID_SCALE) + '/' + str(NUMBER_OF_EPOCHS),\n",
    "    tracking_uri=\"http://127.0.0.1:8080\",\n",
    "    save_dir=\"MLFlow/mlruns\",\n",
    "    log_model=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Experiment with name FOMO_Crossroad not found. Creating it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | _network       | Sequential | 68.2 K\n",
      "1 | _loss_function | DiceLoss   | 0     \n",
      "----------------------------------------------\n",
      "68.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "68.2 K    Total params\n",
      "0.273     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 13/13 [00:10<00:00,  1.19it/s, v_num=8e1b, train_loss=0.987, valid_loss=0.988]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 13/13 [00:10<00:00,  1.19it/s, v_num=8e1b, train_loss=0.987, valid_loss=0.988]\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=NUMBER_OF_EPOCHS,\n",
    "    logger=mlflow_logger,\n",
    "    log_every_n_steps=1,\n",
    "    default_root_dir=\"Checkpoints\"\n",
    ")\n",
    "\n",
    "trainer.fit(model=fomo_module, datamodule=vehicle_data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect dataset images and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_dataset(data_idx: int, class_idx: int):\n",
    "    dataset = VehicleDataset(\n",
    "                DATASET_DIR / Path(\"training\"),\n",
    "                vehicle_data_module._transform,\n",
    "                vehicle_data_module._target_transform,\n",
    "    )\n",
    "    image, mask, count = dataset[data_idx]\n",
    "\n",
    "    image = image.permute(1, 2, 0).numpy().astype(int)\n",
    "    class_mask = mask[class_idx]\n",
    "    mask_res = cv2.resize(\n",
    "        class_mask.numpy(),\n",
    "        dsize=(IMAGE_RESOLUTION, IMAGE_RESOLUTION),\n",
    "        interpolation=cv2.INTER_NEAREST,\n",
    "    )\n",
    "    image[mask_res == 1] = [0, 0, 255]\n",
    "\n",
    "    plt.figure(figsize = (10,10))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "    print(str(count[0]), \"object(s) in the picture\")\n",
    "\n",
    "inspect_dataset(data_idx=6, class_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'Checkpoints/373381907947535324/3c2a8ac78ee341d2b6634d4574e9db5d/checkpoints/epoch=0-step=13.ckpt'\n",
    "\n",
    "FOMO_model = FomoModule.load_from_checkpoint(MODEL_PATH)\n",
    "\n",
    "result = trainer.test(FOMO_model, datamodule=vehicle_data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_display(mask: torch.Tensor):\n",
    "    mask_res = cv2.resize(\n",
    "        mask.numpy().squeeze(),\n",
    "        dsize=(IMAGE_RESOLUTION, IMAGE_RESOLUTION),\n",
    "        interpolation=cv2.INTER_NEAREST,\n",
    "    )\n",
    "    return mask_res\n",
    "\n",
    "def test_model(model_path: str, data_idx: int, mask_idx: int, threshold: float):\n",
    "    dataset = VehicleDataset(\n",
    "                DATASET_DIR / Path(\"training\"),\n",
    "                vehicle_data_module._transform,\n",
    "                vehicle_data_module._target_transform,\n",
    "    )\n",
    "    image, mask, count = dataset[data_idx]\n",
    "    model = FomoModule.load_from_checkpoint(model_path)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        mask_pred = model(image.unsqueeze(0).cuda()).cpu()\n",
    "        mask_post_sig, cnt = postprocess_prediction(mask_pred, threshold=threshold, binary=False, connected=False)\n",
    "        mask_post_bin, cnt = postprocess_prediction(mask_pred, threshold=threshold, binary=True, connected=False)\n",
    "        mask_post_con, cnt= postprocess_prediction(mask_pred, threshold=threshold, binary=True, connected=True)\n",
    "    \n",
    "    print('cnt:', cnt)\n",
    "    image = image.permute(1, 2, 0).numpy().astype(int)\n",
    "    mask_result = mask_to_display(mask_post_con)\n",
    "    mask_ref = mask_to_display(mask[mask_idx])\n",
    "    mask_check = np.logical_and(mask_result==1, mask_ref==1)\n",
    "    image[mask_result==1] = [0, 0, 255]     # blue\n",
    "    # image[mask_ref==1]    = [255, 0, 0]     # red\n",
    "    # image[mask_check==1]  = [255, 0, 255]   # purple\n",
    "\n",
    "    _, axs = plt.subplots(2, 2, figsize=(20, 20))\n",
    "    for ax in axs.ravel():\n",
    "        ax.set_axis_off()\n",
    "    axs[0][0].imshow(image)\n",
    "    image[mask_result==1] = [0, 0, 255]\n",
    "    axs[0][1].imshow(mask_to_display(mask_post_bin))\n",
    "    axs[1][0].imshow(mask_to_display(mask_post_bin))\n",
    "    axs[1][1].imshow(mask_to_display(mask_post_con))\n",
    "\n",
    "    count_pred = np.count_nonzero(mask_result)//(pow(GRID_SCALE, 2))\n",
    "    print(str(count_pred), \" from \" + str(count[0]) + \" objects in the picture\")\n",
    "\n",
    "test_model(model_path=\"219408738261620929/9ea9c07e92c34661a62a0dda2712c21a/checkpoints/epoch=49-step=500.ckpt\",\n",
    "           data_idx=5, mask_idx=0, threshold=DETECTION_THRESHOLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_fomo_model():\n",
    "    model = FomoModule.load_from_checkpoint(\n",
    "        \"399010269064744334/56634d5296d1445793f845250bb3c4e0/checkpoints/epoch=9-step=540.ckpt\"\n",
    "    )\n",
    "\n",
    "    model.to_onnx(\n",
    "        \"fomo.onnx\",\n",
    "        export_params=True,\n",
    "        input_names=[\"image\"],\n",
    "        output_names=[\"masks\"],\n",
    "        input_sample=torch.randn(1, 3, IMAGE_RESOLUTION, IMAGE_RESOLUTION),\n",
    "    )\n",
    "\n",
    "export_fomo_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".fomo_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
